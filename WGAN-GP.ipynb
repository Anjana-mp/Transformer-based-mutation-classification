{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9776f3bd-6287-4324-ac91-25bd0e62c73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, BatchNormalization\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sequence_length = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09ecbd61-1725-47f7-9bf6-79e7a5f571b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGenerator(tf.keras.Model):\n",
    "    def __init__(self, sequence_length, num_nucleotides, num_hidden_units=512):\n",
    "        super(LSTMGenerator, self).__init__()\n",
    "        self.lstm1 = Bidirectional(LSTM(num_hidden_units, return_sequences=True))\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.lstm2 = Bidirectional(LSTM(num_hidden_units, return_sequences=True))\n",
    "        self.bn2 = BatchNormalization()\n",
    "        self.dense = Dense(num_nucleotides, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.bn1(self.lstm1(inputs))\n",
    "        x = self.bn2(self.lstm2(x))\n",
    "        return self.dense(x)\n",
    "\n",
    "class LSTMDiscriminator(tf.keras.Model):\n",
    "    def __init__(self, num_hidden_units=512):\n",
    "        super(LSTMDiscriminator, self).__init__()\n",
    "        self.lstm1 = Bidirectional(LSTM(num_hidden_units, return_sequences=True))\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.lstm2 = Bidirectional(LSTM(num_hidden_units, return_sequences=True))\n",
    "        self.bn2 = BatchNormalization()\n",
    "        self.dense = Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.bn1(self.lstm1(inputs))\n",
    "        x = self.bn2(self.lstm2(x))\n",
    "        return self.dense(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35611a4d-9aac-4d3f-b957-b3e851beb7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3987, 31, 4)\n"
     ]
    }
   ],
   "source": [
    "def read_input():\n",
    "  \"\"\" Read the input data \"\"\"\n",
    "  data_path = \"Mutated_Sequence.csv\"\n",
    "  data = pd.read_csv(data_path)\n",
    "  \n",
    "  dataset = data.loc[data['labels'] == \"['DUP']\"]\n",
    "  arr = dataset.values\n",
    "  sequences = arr[:,0]\n",
    "  \n",
    "  sequences = [x.upper() for x in sequences]\n",
    "  \n",
    "  rna_vocab = {\"A\":0,\n",
    "               \"C\":1,\n",
    "               \"G\":2,\n",
    "               \"T\":3}\n",
    "  rev_rna_vocab = {v:k for k,v in rna_vocab.items()}\n",
    "  return sequences\n",
    "\n",
    "\n",
    "def one_hot_encode(seq, SEQ_LEN=31):\n",
    "    mapping = {\"A\": 0, \"C\": 1, \"G\": 2, \"T\": 3}\n",
    "    seq2 = [mapping[i] for i in seq]\n",
    "    encoded = np.zeros((SEQ_LEN, 4))  # Ensure shape consistency\n",
    "    for i, nucleotide in enumerate(seq2[:SEQ_LEN]):\n",
    "        encoded[i, nucleotide] = 1\n",
    "    return encoded\n",
    "\n",
    "sequences = read_input()\n",
    "\n",
    "\n",
    "encoded_sequences = np.asarray([one_hot_encode(x) for x in sequences], dtype=np.float32)\n",
    "\n",
    "print(encoded_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d6e0581-995a-41a2-ae1e-dde1d3425ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tf.data.Dataset\n",
    "batch_size = 32\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded_sequences)\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size=len(encoded_sequences)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63bcc322-c28d-4113-8a8a-3fcc92e23191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "def generator_loss(fake_output):\n",
    "    return -tf.reduce_mean(fake_output)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = -tf.reduce_mean(real_output)  # Want real samples to be classified as positive\n",
    "    fake_loss = tf.reduce_mean(fake_output)   # Want fake samples to be classified as negative\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "\n",
    "# Gradient Penalty (Fixed: Removed Clipping)\n",
    "def gradient_penalty(real_data, fake_data):\n",
    "    batch_size = tf.shape(real_data)[0]\n",
    "    alpha = tf.random.uniform([batch_size, 1, 1], 0.0, 1.0)\n",
    "    interpolated = alpha * real_data + (1 - alpha) * fake_data\n",
    "\n",
    "    with tf.GradientTape() as gp_tape:\n",
    "        gp_tape.watch(interpolated)\n",
    "        interpolated_output = discriminator(interpolated)\n",
    "\n",
    "    grads = gp_tape.gradient(interpolated_output, [interpolated])[0]\n",
    "    grads_norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2]))\n",
    "\n",
    "    # Corrected WGAN-GP gradient penalty\n",
    "    return tf.reduce_mean((grads_norm - 1.0) ** 2)\n",
    "\n",
    "def gc_loss(probabilities, gc_target=0.5):\n",
    "    \"\"\"\n",
    "    Compute GC content regularization loss for generated sequences.\n",
    "\n",
    "    Args:\n",
    "        probabilities: Tensor of shape [batch_size, sequence_length, 4] (one-hot encoded)\n",
    "        gc_target: Target GC content (default: 50%)\n",
    "\n",
    "    Returns:\n",
    "        GC content penalty loss\n",
    "    \"\"\"\n",
    "    # One-hot index mapping: [A, T, C, G] â†’ C is index 2, G is index 3\n",
    "    gc_probs = probabilities[..., 1] + probabilities[..., 2]  # Sum of C and G probabilities\n",
    "\n",
    "    # Compute mean GC content per sequence\n",
    "    gc_content = tf.reduce_mean(gc_probs, axis=1)  # Shape: [batch_size]\n",
    "\n",
    "    # Penalize deviation from the target GC content\n",
    "    return tf.reduce_mean(tf.abs(gc_content - gc_target))\n",
    "\n",
    "\n",
    "# Convert probabilities to nucleotide sequences\n",
    "def probabilities_to_nucleotides(probabilities):\n",
    "    nucleotides = ['A', 'T', 'C', 'G']\n",
    "    sequences = []\n",
    "\n",
    "    for prob_matrix in probabilities:\n",
    "        prob_matrix = np.array(prob_matrix)\n",
    "        row_sums = np.sum(prob_matrix, axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1  # Avoid division by zero\n",
    "        prob_matrix = prob_matrix / row_sums\n",
    "\n",
    "        sequence = ''.join([nucleotides[np.random.choice(len(nucleotides), p=pos)] for pos in prob_matrix])\n",
    "        sequences.append(sequence)\n",
    "\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6dc142e-7bc9-4e80-bc9b-f8b0bbba3d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def train_step(real_sequences):\n",
    "    batch_size = tf.shape(real_sequences)[0]\n",
    "    noise = tf.random.normal([batch_size, sequence_length, num_nucleotides])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_sequences = generator(noise)\n",
    "        real_output = discriminator(real_sequences)\n",
    "        fake_output = discriminator(generated_sequences)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "        gp = gradient_penalty(real_sequences, generated_sequences)\n",
    "        disc_loss += lambda_gp * gp\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    gen_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss, generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9676512f-813c-4f6c-8bbe-3c58d82d3971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored from ./training_checkpoints-dup\\ckpt-5\n",
      "Latest checkpoint  ./training_checkpoints-dup\\ckpt-5\n",
      "5\n",
      "Epoch 6, Generator Loss: 0.0784, Discriminator Loss: 0.0185\n",
      "Saved checkpoint for epoch 6\n",
      "Epoch 7, Generator Loss: 0.1199, Discriminator Loss: -0.0341\n",
      "Saved checkpoint for epoch 7\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "# Hyperparameters\n",
    "sequence_length = 31\n",
    "num_nucleotides = 4\n",
    "num_hidden_units = 512\n",
    "batch_size = 32  # Increased for better stability\n",
    "epochs = 150\n",
    "lambda_gp = 10  # Reduced from 10 to 5 for better balance\n",
    "\n",
    "\n",
    "# Model and Optimizers\n",
    "generator = LSTMGenerator(sequence_length, num_nucleotides, num_hidden_units)\n",
    "discriminator = LSTMDiscriminator(num_hidden_units)\n",
    "gen_optimizer = Adam(learning_rate=1e-5, beta_1=0.5, beta_2=0.9)\n",
    "disc_optimizer = Adam(learning_rate=1e-5, beta_1=0.5, beta_2=0.9)\n",
    "\n",
    "# Training Loop\n",
    "def train():\n",
    "    generator = LSTMGenerator(sequence_length, num_nucleotides, num_hidden_units)\n",
    "    discriminator = LSTMDiscriminator(num_hidden_units)\n",
    "    \n",
    "    gen_optimizer = Adam(learning_rate=1e-5, beta_1=0.5, beta_2=0.9)\n",
    "    disc_optimizer = Adam(learning_rate=1e-5, beta_1=0.5, beta_2=0.9)\n",
    "\n",
    "    checkpoint_dir = './training_checkpoints-dup'\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "    \n",
    "    checkpoint = tf.train.Checkpoint(generator_optimizer=gen_optimizer,\n",
    "                                     discriminator_optimizer=disc_optimizer,\n",
    "                                     generator=generator,\n",
    "                                     discriminator=discriminator)\n",
    "\n",
    "    manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=5)\n",
    "\n",
    "    if manager.latest_checkpoint:\n",
    "        checkpoint.restore(manager.latest_checkpoint)\n",
    "        print(f\"Restored from {manager.latest_checkpoint}\")\n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "\n",
    "    gen_loss_list = []\n",
    "    disc_loss_list = []\n",
    "    generated_samples = []\n",
    "    real_samples = []\n",
    "\n",
    "    start_epoch = int(manager.latest_checkpoint.split('-')[-1]) if manager.latest_checkpoint else 0\n",
    "    print(\"Latest checkpoint \", manager.latest_checkpoint)\n",
    "    print(start_epoch)\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        total_gen_loss = 0.0\n",
    "        total_disc_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for real_sequences_batch in dataset:\n",
    "            real_sequences_batch = tf.convert_to_tensor(real_sequences_batch, dtype=tf.float32)\n",
    "            gen_loss, disc_loss, generated_sequences = train_step(real_sequences_batch)\n",
    "\n",
    "            total_gen_loss += gen_loss\n",
    "            total_disc_loss += disc_loss\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_gen_loss = total_gen_loss / num_batches\n",
    "        avg_disc_loss = total_disc_loss / num_batches\n",
    "        gen_loss_list.append(avg_gen_loss)\n",
    "        disc_loss_list.append(avg_disc_loss)\n",
    "\n",
    "        generated_probabilities = generated_sequences.numpy()\n",
    "        nucleotide_sequences = probabilities_to_nucleotides(generated_probabilities)\n",
    "        real = probabilities_to_nucleotides(real_sequences_batch.numpy())\n",
    "        generated_samples.append(nucleotide_sequences)\n",
    "        real_samples.append(real)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Generator Loss: {avg_gen_loss:.4f}, Discriminator Loss: {avg_disc_loss:.4f}')\n",
    "\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "            print(f'Saved checkpoint for epoch {epoch + 1}')\n",
    "\n",
    "\n",
    "        # Save the generated samples to a text file\n",
    "        with open('real_samples-dup.txt', 'a') as f:\n",
    "            for samples in real_samples:\n",
    "                for sample in samples:\n",
    "                    f.write(f'{sample}\\n')\n",
    "\n",
    "        with open('generated_samples-dup.txt', 'a') as f:\n",
    "            for samples in generated_samples:\n",
    "                for sample in samples:\n",
    "                    f.write(f'{sample}\\n')\n",
    "\n",
    "train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
